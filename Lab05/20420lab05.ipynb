{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Oll3m52HH0n",
        "outputId": "f06af430-d2c4-4b6e-8baa-921edc1ec557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents: 2000\n",
            "Number of labels: 2000\n",
            "Target names (classes): ['neg', 'pos']\n",
            "First document preview: arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \n",
            "it's hard seeing arnold as mr ....\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import re\n",
        "from sklearn.datasets import load_files\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load the dataset\n",
        "movie_data = load_files(r\"/content/drive/MyDrive/Colab Notebooks/movie_reviews\")\n",
        "X, y = movie_data.data, movie_data.target\n",
        "\n",
        "# Display dataset summary\n",
        "print(f\"Number of documents: {len(X)}\")\n",
        "print(f\"Number of labels: {len(y)}\")\n",
        "print(f\"Target names (classes): {movie_data.target_names}\")\n",
        "print(f\"First document preview: {X[0].decode('utf-8')[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9obQYWDXHr-_"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "294e0ce1",
        "outputId": "30555162-4558-45b3-8c59-7808afe1984e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing complete. Sample processed document: arnold schwarzenegger ha been an icon for action enthusiast since the late but lately his film have been very sloppy and the one liner are getting worse it hard seeing arnold a mr freeze in batman and...\n"
          ]
        }
      ],
      "source": [
        "documents = []\n",
        "for i in range(len(X)):\n",
        "    # 1. Decode from bytes to string\n",
        "    document = X[i].decode('utf-8')\n",
        "\n",
        "    # 2. Apply regex substitutions\n",
        "    document = re.sub(r'\\W', ' ', document)  # Remove special characters\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)  # Single chars at beginning\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)  # Single chars in middle\n",
        "    document = re.sub(r'\\d+', '', document)  # Remove numbers\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)  # Multiple spaces to one\n",
        "\n",
        "    # 3. Convert to lowercase\n",
        "    document = document.lower()\n",
        "\n",
        "    # 4. Tokenize\n",
        "    document = document.split()\n",
        "\n",
        "    # 5. Lemmatize (reduce words to root form)\n",
        "    document = [lemmatizer.lemmatize(word) for word in document]\n",
        "\n",
        "    # 6. Rejoin tokens\n",
        "    document = ' '.join(document)\n",
        "\n",
        "    # 7. Append to processed documents\n",
        "    documents.append(document)\n",
        "\n",
        "print(f\"Preprocessing complete. Sample processed document: {documents[0][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wThRa6wyHzC2"
      },
      "source": [
        "### Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNdDw76RHwDp",
        "outputId": "f29d9886-a501-4c91-c754-232732eb1278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorized data shape: (2000, 1500)\n",
            "Sample features: ['ability', 'able', 'absolutely', 'academy', 'accent', 'accident', 'across', 'act', 'acting', 'action', 'actor', 'actress', 'actual', 'actually', 'ad', 'adam', 'adaptation', 'add', 'added', 'addition']\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer(\n",
        "    max_features=1500,  # Keep only top 1500 most frequent words\n",
        "    min_df=7,          # Word must appear in at least 7 documents\n",
        "    max_df=0.8,        # Word must appear in less than 80% of documents\n",
        "    stop_words=stopwords.words('english')  # Remove common English stop words\n",
        ")\n",
        "\n",
        "X_vectors = vectorizer.fit_transform(documents).toarray()\n",
        "print(f\"Vectorized data shape: {X_vectors.shape}\")\n",
        "print(f\"Sample features: {list(vectorizer.get_feature_names_out())[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5k7SAonIzbx"
      },
      "source": [
        "### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_hvU0oLH2op",
        "outputId": "d788c566-5f15-471c-b956-1b2b31890c95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Training Logistic Regression...\n",
            "\n",
            "Logistic Regression Results:\n",
            "Accuracy: 0.8200\n",
            "\n",
            "Confusion Matrix:\n",
            "[[164  44]\n",
            " [ 28 164]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.85      0.79      0.82       208\n",
            "         pos       0.79      0.85      0.82       192\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "\n",
            "==================================================\n",
            "Training Random Forest...\n",
            "\n",
            "Random Forest Results:\n",
            "Accuracy: 0.8275\n",
            "\n",
            "Confusion Matrix:\n",
            "[[168  40]\n",
            " [ 29 163]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.85      0.81      0.83       208\n",
            "         pos       0.80      0.85      0.83       192\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.83      0.83      0.83       400\n",
            "weighted avg       0.83      0.83      0.83       400\n",
            "\n",
            "\n",
            "==================================================\n",
            "Training Support Vector Machine...\n",
            "\n",
            "Support Vector Machine Results:\n",
            "Accuracy: 0.8100\n",
            "\n",
            "Confusion Matrix:\n",
            "[[165  43]\n",
            " [ 33 159]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.83      0.79      0.81       208\n",
            "         pos       0.79      0.83      0.81       192\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.81      0.81       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n",
            "\n",
            "==================================================\n",
            "Training Naive Bayes...\n",
            "\n",
            "Naive Bayes Results:\n",
            "Accuracy: 0.8150\n",
            "\n",
            "Confusion Matrix:\n",
            "[[166  42]\n",
            " [ 32 160]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.84      0.80      0.82       208\n",
            "         pos       0.79      0.83      0.81       192\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.82      0.81       400\n",
            "weighted avg       0.82      0.81      0.82       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=0),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=0),\n",
        "    'Support Vector Machine': SVC(kernel='linear', random_state=0),\n",
        "    'Naive Bayes': MultinomialNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}...\")\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': predictions,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "    # Print detailed results\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, predictions, target_names=movie_data.target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYJ42E4fI8fg"
      },
      "source": [
        "### Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZDg4QEtI1YW",
        "outputId": "07d2d2d0-44a7-4057-a21d-3e3178c1c608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL PERFORMANCE COMPARISON\n",
            "============================================================\n",
            "                 Model Accuracy Precision Recall F1-Score\n",
            "   Logistic Regression   0.8200    0.8226 0.8200   0.8200\n",
            "         Random Forest   0.8275    0.8289 0.8275   0.8276\n",
            "Support Vector Machine   0.8100    0.8112 0.8100   0.8101\n",
            "           Naive Bayes   0.8150    0.8162 0.8150   0.8151\n",
            "\n",
            "Best Model: Random Forest with accuracy: 0.8275\n"
          ]
        }
      ],
      "source": [
        "# Create comparison table\n",
        "comparison_data = []\n",
        "for name, result in results.items():\n",
        "    predictions = result['predictions']\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions, average='weighted')\n",
        "\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': f\"{result['accuracy']:.4f}\",\n",
        "        'Precision': f\"{precision:.4f}\",\n",
        "        'Recall': f\"{recall:.4f}\",\n",
        "        'F1-Score': f\"{f1:.4f}\"\n",
        "    })\n",
        "\n",
        "# Display comparison\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
        "print(f\"\\nBest Model: {best_model_name} with accuracy: {results[best_model_name]['accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5k3WBtPJ2Zi"
      },
      "source": [
        "New Data Pre Processings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6mU7YUtJ7uj",
        "outputId": "e2029506-f60c-4eaf-84e5-35d2056c5458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGbgIkuMI-WB",
        "outputId": "ee1b7fb2-6fd0-4efd-f722-c457119457dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust enhanced preprocessing complete.\n",
            " Sample processed document: arnold schwarzenegger icon action enthusiast since late lately film sloppy one liner getting worse hard seeing arnold mr freeze batman robin especially say ton ice joke hey got million matter arnold s...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download only the essential NLTK resources that work across all versions\n",
        "try:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def robust_enhanced_preprocessing(X):\n",
        "    \"\"\"Robust enhanced preprocessing that works across all NLTK versions\"\"\"\n",
        "    documents = []\n",
        "\n",
        "    # Get English stop words\n",
        "    try:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    except:\n",
        "        # Fallback stop words if NLTK fails\n",
        "        stop_words = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
        "                     'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
        "                     'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
        "                     'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
        "                     'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
        "                     'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "                     'while', 'of', 'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after',\n",
        "                     'above', 'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
        "                     'further', 'then', 'once'}\n",
        "\n",
        "    # Preserve important negation words\n",
        "    negation_words = {'not', 'no', 'never', 'neither', 'nor', 'none', 'nobody', 'nothing', 'nowhere'}\n",
        "    stop_words = stop_words - negation_words\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        # 1. Decode from bytes to string\n",
        "        document = X[i].decode('utf-8')\n",
        "\n",
        "        # 2. Basic HTML tag removal (simple regex approach)\n",
        "        document = re.sub(r'<[^>]+>', ' ', document)\n",
        "\n",
        "        # 3. Handle contractions manually (most common ones)\n",
        "        contractions_dict = {\n",
        "            \"don't\": \"do not\", \"won't\": \"will not\", \"can't\": \"cannot\",\n",
        "            \"n't\": \" not\", \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\",\n",
        "            \"'d\": \" would\", \"'m\": \" am\", \"it's\": \"it is\", \"that's\": \"that is\",\n",
        "            \"there's\": \"there is\", \"here's\": \"here is\", \"what's\": \"what is\",\n",
        "            \"where's\": \"where is\", \"how's\": \"how is\", \"let's\": \"let us\"\n",
        "        }\n",
        "\n",
        "        for contraction, expansion in contractions_dict.items():\n",
        "            document = document.replace(contraction, expansion)\n",
        "\n",
        "        # 4. Handle negation (add NOT_ prefix to words after negation)\n",
        "        document = handle_negation_robust(document)\n",
        "\n",
        "        # 5. Enhanced regex cleaning\n",
        "        document = re.sub(r'[^\\w\\s!?]', ' ', document)  # Keep ! and ? for emotion\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)  # Single chars at beginning\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)  # Single chars in middle\n",
        "        document = re.sub(r'\\d+', '', document)  # Remove numbers\n",
        "        document = re.sub(r'!+', '!', document)  # Multiple ! to single !\n",
        "        document = re.sub(r'\\?+', '?', document)  # Multiple ? to single ?\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)  # Multiple spaces to one\n",
        "\n",
        "        # 6. Convert to lowercase\n",
        "        document = document.lower()\n",
        "\n",
        "        # 7. Tokenize\n",
        "        tokens = document.split()\n",
        "\n",
        "        # 8. Filter word length (remove very short/long words)\n",
        "        tokens = [word for word in tokens if 2 <= len(word) <= 15]\n",
        "\n",
        "        # 9. Remove stop words (preserving negation words)\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # 10. Lemmatize with error handling\n",
        "        lemmatized_tokens = []\n",
        "        for word in tokens:\n",
        "            try:\n",
        "                lemmatized_tokens.append(lemmatizer.lemmatize(word))\n",
        "            except:\n",
        "                lemmatized_tokens.append(word)  # Keep original if lemmatization fails\n",
        "\n",
        "        # 11. Join tokens back\n",
        "        document = ' '.join(lemmatized_tokens)\n",
        "\n",
        "        # 12. Final cleanup and append\n",
        "        document = document.strip()\n",
        "        if document:  # Only add non-empty documents\n",
        "            documents.append(document)\n",
        "        else:\n",
        "            documents.append(\"empty\")  # Placeholder for empty documents\n",
        "\n",
        "    return documents\n",
        "\n",
        "def handle_negation_robust(text):\n",
        "    \"\"\"Robust negation handling without external dependencies\"\"\"\n",
        "    negation_words = ['not', 'no', 'never', 'neither', 'nor', 'none', 'nobody', 'nothing', 'nowhere']\n",
        "    tokens = text.split()\n",
        "\n",
        "    result = []\n",
        "    negate = False\n",
        "\n",
        "    for token in tokens:\n",
        "        clean_token = re.sub(r'[^\\w]', '', token.lower())  # Remove punctuation for comparison\n",
        "\n",
        "        if clean_token in negation_words:\n",
        "            negate = True\n",
        "            result.append(token)\n",
        "        elif negate and clean_token.isalpha() and len(clean_token) > 1:\n",
        "            result.append(f\"NOT_{clean_token}\")\n",
        "            negate = False\n",
        "        else:\n",
        "            result.append(token)\n",
        "            # Reset negation at sentence boundaries\n",
        "            if token.endswith(('.', '!', '?', ';')):\n",
        "                negate = False\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Usage - This will work without any NLTK compatibility issues\n",
        "documents = robust_enhanced_preprocessing(X)\n",
        "print(f\"Robust enhanced preprocessing complete.\\n Sample processed document: {documents[0][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR7yJQ0h6g9A"
      },
      "source": [
        "## After Using Different Pre-Processing Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8I1inCBJ4G1",
        "outputId": "71943baa-d9c3-4135-e97e-a0a6fdbba243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorized data shape: (2000, 1500)\n",
            "Sample features: ['ability', 'able', 'absolutely', 'academy', 'accent', 'accident', 'across', 'act', 'acting', 'action', 'actor', 'actress', 'actual', 'actually', 'adam', 'adaptation', 'add', 'added', 'addition', 'admit']\n",
            "\n",
            "==================================================\n",
            "Training Logistic Regression...\n",
            "\n",
            "Logistic Regression Results:\n",
            "Accuracy: 0.8100\n",
            "\n",
            "Confusion Matrix:\n",
            "[[160  48]\n",
            " [ 28 164]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.85      0.77      0.81       208\n",
            "         pos       0.77      0.85      0.81       192\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.81      0.81       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n",
            "\n",
            "==================================================\n",
            "Training Random Forest...\n",
            "\n",
            "Random Forest Results:\n",
            "Accuracy: 0.8200\n",
            "\n",
            "Confusion Matrix:\n",
            "[[160  48]\n",
            " [ 24 168]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.87      0.77      0.82       208\n",
            "         pos       0.78      0.88      0.82       192\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.83      0.82      0.82       400\n",
            "\n",
            "\n",
            "==================================================\n",
            "Training Support Vector Machine...\n",
            "\n",
            "Support Vector Machine Results:\n",
            "Accuracy: 0.7925\n",
            "\n",
            "Confusion Matrix:\n",
            "[[162  46]\n",
            " [ 37 155]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.81      0.78      0.80       208\n",
            "         pos       0.77      0.81      0.79       192\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.79       400\n",
            "weighted avg       0.79      0.79      0.79       400\n",
            "\n",
            "\n",
            "==================================================\n",
            "Training Naive Bayes...\n",
            "\n",
            "Naive Bayes Results:\n",
            "Accuracy: 0.8200\n",
            "\n",
            "Confusion Matrix:\n",
            "[[167  41]\n",
            " [ 31 161]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.84      0.80      0.82       208\n",
            "         pos       0.80      0.84      0.82       192\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "\n",
            "============================================================\n",
            "MODEL PERFORMANCE COMPARISON\n",
            "============================================================\n",
            "                 Model Accuracy Precision Recall F1-Score\n",
            "   Logistic Regression   0.8100    0.8139 0.8100   0.8099\n",
            "         Random Forest   0.8200    0.8255 0.8200   0.8198\n",
            "Support Vector Machine   0.7925    0.7935 0.7925   0.7926\n",
            "           Naive Bayes   0.8200    0.8212 0.8200   0.8201\n",
            "\n",
            "Best Model: Random Forest with accuracy: 0.8200\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=1500,  # Keep only top 1500 most frequent words\n",
        "    min_df=7,          # Word must appear in at least 7 documents\n",
        "    max_df=0.8,        # Word must appear in less than 80% of documents\n",
        "    stop_words=stopwords.words('english')  # Remove common English stop words\n",
        ")\n",
        "\n",
        "X_vectors = vectorizer.fit_transform(documents).toarray()\n",
        "print(f\"Vectorized data shape: {X_vectors.shape}\")\n",
        "print(f\"Sample features: {list(vectorizer.get_feature_names_out())[:20]}\")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=0),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=0),\n",
        "    'Support Vector Machine': SVC(kernel='linear', random_state=0),\n",
        "    'Naive Bayes': MultinomialNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}...\")\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': predictions,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "    # Print detailed results\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, predictions))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, predictions, target_names=movie_data.target_names))\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = []\n",
        "for name, result in results.items():\n",
        "    predictions = result['predictions']\n",
        "\n",
        "    # Calculate detailed metrics\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions, average='weighted')\n",
        "\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': f\"{result['accuracy']:.4f}\",\n",
        "        'Precision': f\"{precision:.4f}\",\n",
        "        'Recall': f\"{recall:.4f}\",\n",
        "        'F1-Score': f\"{f1:.4f}\"\n",
        "    })\n",
        "\n",
        "# Display comparison\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
        "print(f\"\\nBest Model: {best_model_name} with accuracy: {results[best_model_name]['accuracy']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
